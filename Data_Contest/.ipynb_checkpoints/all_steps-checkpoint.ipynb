{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ml_metrics as metrics\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"ask\")\n",
    "from geopy.distance import distance\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=0.01, max_retries = 10)\n",
    "from tqdm import tqdm\n",
    "import pycountry\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "test = pd.read_csv(\"Data/test.csv\")\n",
    "biker = pd.read_csv(\"Data/bikers.csv\")\n",
    "biker_net = pd.read_csv(\"Data/bikers_network.csv\")\n",
    "tours = pd.read_csv(\"Data/tours.csv\")\n",
    "tour_convoy = pd.read_csv(\"Data/tour_convoy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bikers_set = np.array(train[\"biker_id\"].drop_duplicates())\n",
    "test_bikers_set = np.array(test[\"biker_id\"].drop_duplicates())\n",
    "req_bikers_set = np.union1d(train_bikers_set, test_bikers_set)\n",
    "tours_set = np.array(pd.merge(test[\"tour_id\"].drop_duplicates(), \n",
    "                              train[\"tour_id\"].drop_duplicates(), how = 'outer'))\n",
    "tours_set = tours_set.reshape((tours_set.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biker = biker[biker.biker_id.isin(req_bikers_set)]\n",
    "tours = tours[tours.tour_id.isin(tours_set)]\n",
    "biker_net = biker_net[biker_net.biker_id.isin(req_bikers_set)]\n",
    "tour_convoy = tour_convoy[tour_convoy.tour_id.isin(tours_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long = {}\n",
    "biker = biker.fillna(\"\")\n",
    "bids, lats, longs = [], [], []\n",
    "for index, row in tqdm(biker.iterrows()):\n",
    "    bid = row[\"biker_id\"]\n",
    "    f = [0]*4\n",
    "    l = row[\"area\"]\n",
    "    if pd.isna(l) or l == None or l == \"\":\n",
    "        l = pycountry.countries.get(alpha_2=row[\"location_id\"]).name\n",
    "    l = ''.join([i for i in l if not i.isdigit()])\n",
    "    location = geolocator.geocode(l, timeout = 100)\n",
    "    if location == None:\n",
    "        lat_long[bid] = (0,0)\n",
    "    else:\n",
    "        lat_long[bid] = (location.latitude, location.longitude)\n",
    "    bids.append(bid)\n",
    "    lats.append(lat_long[bid][0])\n",
    "    longs.append(lat_long[bid][1])\n",
    "print(len(lat_long))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biker[\"latitude\"] = lats\n",
    "#biker[\"longitude\"] = longs\n",
    "dire = \"temp\"\n",
    "biker.to_csv(dire + \"/bikers_useful.csv\", index = False)\n",
    "tours.to_csv(dire + \"/tours_useful.csv\", index = False)\n",
    "biker_net.to_csv(dire + \"/bikers_network_useful.csv\", index = False)\n",
    "tour_convoy.to_csv(dire + \"/tour_convoy_useful.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "test = pd.read_csv(\"Data/test.csv\")\n",
    "biker = { key: row for key, *row in csv.reader(open(dire + \"/bikers_useful.csv\", 'r'))}\n",
    "print(biker.pop('biker_id'))\n",
    "biker_net = { key: row for key, *row in csv.reader(open(dire + \"/bikers_network_useful.csv\", 'r'))}\n",
    "print(biker_net.pop('biker_id'))\n",
    "tours = { key: row for key, *row in csv.reader(open(dire + \"/tours_useful.csv\", 'r'))}\n",
    "print(tours.pop('tour_id'))\n",
    "tour_convoy = { key: row for key, *row in csv.reader(open(dire + \"/tour_convoy_useful.csv\", 'r'))}\n",
    "print(tour_convoy.pop('tour_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bikers_set = np.array(train[\"biker_id\"].drop_duplicates())\n",
    "test_bikers_set = np.array(test[\"biker_id\"].drop_duplicates())\n",
    "req_bikers_set = np.array([*biker])\n",
    "tours_set = np.array([*tours])\n",
    "tour_convoy_full = { key: row for key, *row in csv.reader(open(\"Data/tour_convoy.csv\", 'r'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_attended = {}\n",
    "events_notgoing = {}\n",
    "events_maybe = {}\n",
    "for tid, row in tour_convoy_full.items():\n",
    "    people_attended = row[0].split()\n",
    "    people_maybe = row[1].split()\n",
    "    people_notgoing = row[2].split()\n",
    "    for bid in people_attended:\n",
    "        events_attended[bid] = events_attended.get(bid, 0) + 1\n",
    "    for bid in people_notgoing:\n",
    "        events_notgoing[bid] = events_notgoing.get(bid, 0) + 1\n",
    "    for bid in people_maybe:\n",
    "        events_maybe[bid] = events_maybe.get(bid, 0) + 1\n",
    "        \n",
    "events_attended_friends = {}\n",
    "events_notgoing_friends = {}\n",
    "events_maybe_friends = {}\n",
    "for bid, row in biker_net.items():\n",
    "    friends = row[0].split()\n",
    "    for friend in friends:\n",
    "        events_attended_friends[bid] = events_attended_friends.get(bid, 0) + events_attended.get(friend, 0)\n",
    "        events_notgoing_friends[bid] = events_notgoing_friends.get(bid, 0) + events_notgoing.get(friend, 0)\n",
    "        events_maybe_friends[bid] = events_maybe_friends.get(bid, 0) + events_maybe.get(friend, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta(bid, tid, timestamp):\n",
    "    return (abs(datetime.strptime(timestamp[:10], \"%d-%m-%Y\") - \n",
    "                datetime.strptime(tours[tid][1], \"%d-%m-%Y\")).total_seconds())/1e7\n",
    "\n",
    "def get_distance(bid, tid):\n",
    "    return distance((biker[bid][7], biker[bid][8]), (tours[tid][6], tours[tid][7])).miles\n",
    "\n",
    "def get_top10_wc(tid):\n",
    "    row = tours[tid]\n",
    "    wsum = 0\n",
    "    for i in range(8, 18):\n",
    "        wsum += int(row[i])\n",
    "    return wsum\n",
    "\n",
    "fraction_in = {}\n",
    "total_attendees = {}\n",
    "deltas = {}\n",
    "total_maybe = {}\n",
    "\n",
    "for tid, row in tour_convoy.items():\n",
    "    not_going = len(row[3].split())\n",
    "    invited = len(row[2].split())\n",
    "    maybe = len(row[1].split())\n",
    "    attend = len(row[0].split())\n",
    "    fraction_in[tid] = (not_going)/(invited+1)\n",
    "    total_attendees[tid] = attend\n",
    "    total_maybe[tid] = maybe\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    a = row[\"biker_id\"]\n",
    "    b = row[\"tour_id\"]\n",
    "    c = row[\"timestamp\"]\n",
    "    curr_delta = get_delta(a,b,c)\n",
    "    if a not in deltas:\n",
    "        deltas[a] = curr_delta\n",
    "    else:\n",
    "        deltas[a] = min(deltas[a], curr_delta)\n",
    "        \n",
    "for index, row in test.iterrows():\n",
    "    a = row[\"biker_id\"]\n",
    "    b = row[\"tour_id\"]\n",
    "    c = row[\"timestamp\"]\n",
    "    curr_delta = get_delta(a,b,c)\n",
    "    if a not in deltas:\n",
    "        deltas[a] = curr_delta\n",
    "    else:\n",
    "        deltas[a] = min(deltas[a], curr_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 0\n",
    "def make_feature(bid, tid, timestamp, invited):\n",
    "    f = [0.0]*8\n",
    "    f[0] = get_delta(bid, tid, timestamp)\n",
    "    f[1] = fraction_in[tid]\n",
    "    f[2] = total_attendees[tid]\n",
    "    f[3] = get_distance(bid, tid)\n",
    "    f[4] = events_attended_friends[bid]\n",
    "    f[5] = get_top10_wc(tid)\n",
    "    f[6] = 1 if f[0] == 0 else deltas[bid]/f[0]\n",
    "    f[7] = invited\n",
    "#     f[8] = total_maybe[tid]\n",
    "#     f[9] = events_notgoing_friends[bid]\n",
    "#     f[10] = events_maybe_friends[bid]\n",
    "    return np.array(f)\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ml_metrics as metrics\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"ask\")\n",
    "\n",
    "from geopy.distance import distance\n",
    "\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=0.01, max_retries = 10)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pycountry\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "\n",
    "test = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "biker = pd.read_csv(\"Data/bikers.csv\")\n",
    "\n",
    "biker_net = pd.read_csv(\"Data/bikers_network.csv\")\n",
    "\n",
    "tours = pd.read_csv(\"Data/tours.csv\")\n",
    "\n",
    "tour_convoy = pd.read_csv(\"Data/tour_convoy.csv\")\n",
    "\n",
    "train_bikers_set = np.array(train[\"biker_id\"].drop_duplicates())\n",
    "\n",
    "test_bikers_set = np.array(test[\"biker_id\"].drop_duplicates())\n",
    "\n",
    "req_bikers_set = np.union1d(train_bikers_set, test_bikers_set)\n",
    "\n",
    "tours_set = np.array(pd.merge(test[\"tour_id\"].drop_duplicates(), \n",
    "\n",
    "                              train[\"tour_id\"].drop_duplicates(), how = 'outer'))\n",
    "\n",
    "tours_set = tours_set.reshape((tours_set.shape[0],))\n",
    "\n",
    "biker = biker[biker.biker_id.isin(req_bikers_set)]\n",
    "\n",
    "tours = tours[tours.tour_id.isin(tours_set)]\n",
    "\n",
    "biker_net = biker_net[biker_net.biker_id.isin(req_bikers_set)]\n",
    "\n",
    "tour_convoy = tour_convoy[tour_convoy.tour_id.isin(tours_set)]\n",
    "\n",
    "lat_long = {}\n",
    "\n",
    "biker = biker.fillna(\"\")\n",
    "\n",
    "bids, lats, longs = [], [], []\n",
    "\n",
    "for index, row in tqdm(biker.iterrows()):\n",
    "\n",
    "    bid = row[\"biker_id\"]\n",
    "\n",
    "    f = [0]*4\n",
    "\n",
    "    l = row[\"area\"]\n",
    "\n",
    "    if pd.isna(l) or l == None or l == \"\":\n",
    "\n",
    "        l = pycountry.countries.get(alpha_2=row[\"location_id\"]).name\n",
    "\n",
    "    l = ''.join([i for i in l if not i.isdigit()])\n",
    "\n",
    "    location = geolocator.geocode(l, timeout = 100)\n",
    "\n",
    "    if location == None:\n",
    "\n",
    "        lat_long[bid] = (0,0)\n",
    "\n",
    "    else:\n",
    "\n",
    "        lat_long[bid] = (location.latitude, location.longitude)\n",
    "\n",
    "    bids.append(bid)\n",
    "\n",
    "    lats.append(lat_long[bid][0])\n",
    "\n",
    "    longs.append(lat_long[bid][1])\n",
    "\n",
    "print(len(lat_long))   \n",
    "\n",
    "#biker[\"latitude\"] = lats\n",
    "\n",
    "#biker[\"longitude\"] = longs\n",
    "\n",
    "dire = \"temp\"\n",
    "\n",
    "biker.to_csv(dire + \"/bikers_useful.csv\", index = False)\n",
    "\n",
    "tours.to_csv(dire + \"/tours_useful.csv\", index = False)\n",
    "\n",
    "biker_net.to_csv(dire + \"/bikers_network_useful.csv\", index = False)\n",
    "\n",
    "tour_convoy.to_csv(dire + \"/tour_convoy_useful.csv\", index = False)\n",
    "\n",
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "\n",
    "test = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "biker = { key: row for key, *row in csv.reader(open(dire + \"/bikers_useful.csv\", 'r'))}\n",
    "\n",
    "print(biker.pop('biker_id'))\n",
    "\n",
    "biker_net = { key: row for key, *row in csv.reader(open(dire + \"/bikers_network_useful.csv\", 'r'))}\n",
    "\n",
    "print(biker_net.pop('biker_id'))\n",
    "\n",
    "tours = { key: row for key, *row in csv.reader(open(dire + \"/tours_useful.csv\", 'r'))}\n",
    "\n",
    "print(tours.pop('tour_id'))\n",
    "\n",
    "tour_convoy = { key: row for key, *row in csv.reader(open(dire + \"/tour_convoy_useful.csv\", 'r'))}\n",
    "\n",
    "print(tour_convoy.pop('tour_id'))\n",
    "\n",
    "train_bikers_set = np.array(train[\"biker_id\"].drop_duplicates())\n",
    "\n",
    "test_bikers_set = np.array(test[\"biker_id\"].drop_duplicates())\n",
    "\n",
    "req_bikers_set = np.array([*biker])\n",
    "\n",
    "tours_set = np.array([*tours])\n",
    "\n",
    "tour_convoy_full = { key: row for key, *row in csv.reader(open(\"Data/tour_convoy.csv\", 'r'))}\n",
    "\n",
    "events_attended = {}\n",
    "\n",
    "events_notgoing = {}\n",
    "\n",
    "events_maybe = {}\n",
    "\n",
    "for tid, row in tour_convoy_full.items():\n",
    "\n",
    "    people_attended = row[0].split()\n",
    "\n",
    "    people_maybe = row[1].split()\n",
    "\n",
    "    people_notgoing = row[2].split()\n",
    "\n",
    "    for bid in people_attended:\n",
    "\n",
    "        events_attended[bid] = events_attended.get(bid, 0) + 1\n",
    "\n",
    "    for bid in people_notgoing:\n",
    "\n",
    "        events_notgoing[bid] = events_notgoing.get(bid, 0) + 1\n",
    "\n",
    "    for bid in people_maybe:\n",
    "\n",
    "        events_maybe[bid] = events_maybe.get(bid, 0) + 1\n",
    "\n",
    "        \n",
    "\n",
    "events_attended_friends = {}\n",
    "\n",
    "events_notgoing_friends = {}\n",
    "\n",
    "events_maybe_friends = {}\n",
    "\n",
    "for bid, row in biker_net.items():\n",
    "\n",
    "    friends = row[0].split()\n",
    "\n",
    "    for friend in friends:\n",
    "\n",
    "        events_attended_friends[bid] = events_attended_friends.get(bid, 0) + events_attended.get(friend, 0)\n",
    "\n",
    "        events_notgoing_friends[bid] = events_notgoing_friends.get(bid, 0) + events_notgoing.get(friend, 0)\n",
    "\n",
    "        events_maybe_friends[bid] = events_maybe_friends.get(bid, 0) + events_maybe.get(friend, 0)\n",
    "\n",
    "def get_delta(bid, tid, timestamp):\n",
    "\n",
    "    return (abs(datetime.strptime(timestamp[:10], \"%d-%m-%Y\") - \n",
    "\n",
    "                datetime.strptime(tours[tid][1], \"%d-%m-%Y\")).total_seconds())/1e7\n",
    "\n",
    "​\n",
    "\n",
    "def get_distance(bid, tid):\n",
    "\n",
    "    return distance((biker[bid][7], biker[bid][8]), (tours[tid][6], tours[tid][7])).miles\n",
    "\n",
    "​\n",
    "\n",
    "def get_top10_wc(tid):\n",
    "\n",
    "    row = tours[tid]\n",
    "\n",
    "    wsum = 0\n",
    "\n",
    "    for i in range(8, 18):\n",
    "\n",
    "        wsum += int(row[i])\n",
    "\n",
    "    return wsum\n",
    "\n",
    "​\n",
    "\n",
    "fraction_in = {}\n",
    "\n",
    "total_attendees = {}\n",
    "\n",
    "deltas = {}\n",
    "\n",
    "total_maybe = {}\n",
    "\n",
    "​\n",
    "\n",
    "for tid, row in tour_convoy.items():\n",
    "\n",
    "    not_going = len(row[3].split())\n",
    "\n",
    "    invited = len(row[2].split())\n",
    "\n",
    "    maybe = len(row[1].split())\n",
    "\n",
    "    attend = len(row[0].split())\n",
    "\n",
    "    fraction_in[tid] = (not_going)/(invited+1)\n",
    "\n",
    "    total_attendees[tid] = attend\n",
    "\n",
    "    total_maybe[tid] = maybe\n",
    "\n",
    "​\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "\n",
    "    a = row[\"biker_id\"]\n",
    "\n",
    "    b = row[\"tour_id\"]\n",
    "\n",
    "    c = row[\"timestamp\"]\n",
    "\n",
    "    curr_delta = get_delta(a,b,c)\n",
    "\n",
    "    if a not in deltas:\n",
    "\n",
    "        deltas[a] = curr_delta\n",
    "\n",
    "    else:\n",
    "\n",
    "        deltas[a] = min(deltas[a], curr_delta)\n",
    "\n",
    "        \n",
    "\n",
    "for index, row in test.iterrows():\n",
    "\n",
    "    a = row[\"biker_id\"]\n",
    "\n",
    "    b = row[\"tour_id\"]\n",
    "\n",
    "    c = row[\"timestamp\"]\n",
    "\n",
    "    curr_delta = get_delta(a,b,c)\n",
    "\n",
    "    if a not in deltas:\n",
    "\n",
    "        deltas[a] = curr_delta\n",
    "\n",
    "    else:\n",
    "\n",
    "        deltas[a] = min(deltas[a], curr_delta)\n",
    "\n",
    "d = 0\n",
    "\n",
    "def make_feature(bid, tid, timestamp, invited):\n",
    "\n",
    "    f = [0.0]*8\n",
    "\n",
    "    f[0] = get_delta(bid, tid, timestamp)\n",
    "\n",
    "    f[1] = fraction_in[tid]\n",
    "\n",
    "    f[2] = total_attendees[tid]\n",
    "\n",
    "    f[3] = get_distance(bid, tid)\n",
    "\n",
    "    f[4] = events_attended_friends[bid]\n",
    "\n",
    "    f[5] = get_top10_wc(tid)\n",
    "\n",
    "    f[6] = 1 if f[0] == 0 else deltas[bid]/f[0]\n",
    "\n",
    "    f[7] = invited\n",
    "\n",
    "#     f[8] = total_maybe[tid]\n",
    "\n",
    "#     f[9] = events_notgoing_friends[bid]\n",
    "\n",
    "#     f[10] = events_maybe_friends[bid]\n",
    "\n",
    "    return np.array(f)\n",
    "\n",
    "​\n",
    "\n",
    "X, Y = [], []\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "\n",
    "    a = row[\"biker_id\"]\n",
    "\n",
    "    b = row[\"tour_id\"]\n",
    "\n",
    "    c = row[\"timestamp\"]\n",
    "\n",
    "    d = row[\"invited\"]\n",
    "\n",
    "    X.append(make_feature(a, b, c, d))\n",
    "\n",
    "    if(row[\"like\"] == 1):\n",
    "\n",
    "        Y.append(1)\n",
    "\n",
    "    else:\n",
    "\n",
    "        Y.append(0)\n",
    "\n",
    "        \n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "Y = np.array(Y)\n",
    "\n",
    "​\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "nf = X.shape[1]\n",
    "\n",
    "rfmf = nf - 2\n",
    "\n",
    "gbmf = nf - 3\n",
    "\n",
    "classifiers = [\n",
    "\n",
    "    RandomForestClassifier(max_depth=20, n_estimators=361, max_features=6, max_samples = 0.7, random_state = 0),\n",
    "\n",
    "    GradientBoostingRegressor(max_depth=20, n_estimators=351, max_features=7, random_state = 0)\n",
    "\n",
    "]\n",
    "\n",
    "clf0 = classifiers[0]\n",
    "\n",
    "clf1 = classifiers[1]\n",
    "\n",
    "scores = cross_val_score(clf0, X, Y, cv=5, scoring = 'balanced_accuracy')\n",
    "\n",
    "print(scores)\n",
    "\n",
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "scores = cross_val_score(clf1, X, Y, cv=5, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "print(scores)\n",
    "\n",
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "clf0.fit(X,Y)\n",
    "\n",
    "clf1.fit(X,Y)\n",
    "\n",
    "feature_names = ['delta', 'not_going_ratio', 'attended', 'distance', 'eaf', 'word_count', \n",
    "\n",
    "                 'min_delta_ratio', 'invited', 'maybe', 'enf', 'emf']\n",
    "\n",
    "​\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "\n",
    "    X, Y, test_size=0.2, random_state=13)\n",
    "\n",
    "​\n",
    "\n",
    "params = { 'max_depth':20, 'n_estimators':151, 'max_features':7, 'random_state':1 }\n",
    "\n",
    "#params = { 'max_depth':20, 'n_estimators':161, 'max_features':9, 'random_state':1 , 'max_samples':0.7}\n",
    "\n",
    "​\n",
    "\n",
    "#reg = RandomForestClassifier(**params)\n",
    "\n",
    "reg = GradientBoostingRegressor(**params)\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "​\n",
    "\n",
    "'''\n",
    "\n",
    "mse = mean_squared_error(y_test, reg.predict(X_test))\n",
    "\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "​\n",
    "\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(reg.staged_predict(X_test)):\n",
    "\n",
    "    test_score[i] = reg.loss_(y_test, y_pred)\n",
    "\n",
    "​\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "plt.title('Deviance')\n",
    "\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, reg.train_score_, 'b-',\n",
    "\n",
    "         label='Training Set Deviance')\n",
    "\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "\n",
    "         label='Test Set Deviance')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.xlabel('Boosting Iterations')\n",
    "\n",
    "plt.ylabel('Deviance')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show() '''\n",
    "\n",
    "​\n",
    "\n",
    "feature_importance = reg.feature_importances_\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "\n",
    "plt.yticks(pos, np.array(feature_names)[sorted_idx])\n",
    "\n",
    "plt.title('Feature Importance (MDI)')\n",
    "\n",
    "​\n",
    "\n",
    "result = permutation_importance(reg, X_test, y_test, n_repeats=10,\n",
    "\n",
    "                                random_state=42, n_jobs=2)\n",
    "\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.boxplot(result.importances[sorted_idx].T,\n",
    "\n",
    "            vert=False, labels=np.array(feature_names)[sorted_idx])\n",
    "\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "ts = {}\n",
    "\n",
    "inv = {}\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "\n",
    "    ts[(row[\"biker_id\"], row[\"tour_id\"])] = row[\"timestamp\"]\n",
    "\n",
    "    inv[(row[\"biker_id\"], row[\"tour_id\"])] = row[\"invited\"]\n",
    "\n",
    "bikers_out = []\n",
    "\n",
    "tours_out = []\n",
    "\n",
    "for biker1 in tqdm(test_bikers_set):\n",
    "\n",
    "    idx = np.where(biker1==test[\"biker_id\"]) \n",
    "\n",
    "    tour = list(test[\"tour_id\"].loc[idx]) # for each unique biker in test data get all the events  \n",
    "\n",
    "    score = {}\n",
    "\n",
    "    for tou in tour:\n",
    "\n",
    "        fe = make_feature(biker1, tou, ts[(biker1, tou)], inv[(biker1, tou)]).reshape(1,-1)\n",
    "\n",
    "        s1 = clf0.predict_proba(fe)[0][1]\n",
    "\n",
    "        s2 = clf1.predict(fe)[0]\n",
    "\n",
    "        #print(s1, s2)\n",
    "\n",
    "        score[tou] = (s1 + s2)/2\n",
    "\n",
    "    #print(score)\n",
    "\n",
    "    tour.sort(key = lambda x : score[x], reverse = True)\n",
    "\n",
    "    tour = \" \".join(tour) # list to space delimited string\n",
    "\n",
    "    bikers_out.append(biker1)\n",
    "\n",
    "    tours_out.append(tour)\n",
    "\n",
    "sample_submission =pd.DataFrame(columns=[\"biker_id\",\"tour_id\"])\n",
    "\n",
    "sample_submission[\"biker_id\"] = bikers_out\n",
    "\n",
    "sample_submission[\"tour_id\"] = tours_out\n",
    "\n",
    "sample_submission.to_csv(dire + \"/submission.csv\",index=False) # download this file from /kaggle/working directory\n",
    "\n",
    "print(sample_submission.shape)\n",
    "\n",
    "print(sample_submission.head(10))\n",
    "\n",
    "​\n",
    "\n",
    "\n",
    "\n",
    "X, Y = [], []\n",
    "for index, row in train.iterrows():\n",
    "    a = row[\"biker_id\"]\n",
    "    b = row[\"tour_id\"]\n",
    "    c = row[\"timestamp\"]\n",
    "    d = row[\"invited\"]\n",
    "    X.append(make_feature(a, b, c, d))\n",
    "    if(row[\"like\"] == 1):\n",
    "        Y.append(1)\n",
    "    else:\n",
    "        Y.append(0)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = X.shape[1]\n",
    "rfmf = nf - 2\n",
    "gbmf = nf - 3\n",
    "classifiers = [\n",
    "    RandomForestClassifier(max_depth=20, n_estimators=361, max_features=6, max_samples = 0.7, random_state = 0),\n",
    "    GradientBoostingRegressor(max_depth=20, n_estimators=351, max_features=7, random_state = 0)\n",
    "]\n",
    "clf0 = classifiers[0]\n",
    "clf1 = classifiers[1]\n",
    "scores = cross_val_score(clf0, X, Y, cv=5, scoring = 'balanced_accuracy')\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n",
    "scores = cross_val_score(clf1, X, Y, cv=5, scoring = 'neg_mean_squared_error')\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n",
    "clf0.fit(X,Y)\n",
    "clf1.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['delta', 'not_going_ratio', 'attended', 'distance', 'eaf', 'word_count', \n",
    "                 'min_delta_ratio', 'invited', 'maybe', 'enf', 'emf']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=13)\n",
    "\n",
    "params = { 'max_depth':20, 'n_estimators':151, 'max_features':7, 'random_state':1 }\n",
    "#params = { 'max_depth':20, 'n_estimators':161, 'max_features':9, 'random_state':1 , 'max_samples':0.7}\n",
    "\n",
    "#reg = RandomForestClassifier(**params)\n",
    "reg = GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "'''\n",
    "mse = mean_squared_error(y_test, reg.predict(X_test))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "for i, y_pred in enumerate(reg.staged_predict(X_test)):\n",
    "    test_score[i] = reg.loss_(y_test, y_pred)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, reg.train_score_, 'b-',\n",
    "         label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')\n",
    "fig.tight_layout()\n",
    "plt.show() '''\n",
    "\n",
    "feature_importance = reg.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, np.array(feature_names)[sorted_idx])\n",
    "plt.title('Feature Importance (MDI)')\n",
    "\n",
    "result = permutation_importance(reg, X_test, y_test, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(result.importances[sorted_idx].T,\n",
    "            vert=False, labels=np.array(feature_names)[sorted_idx])\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = {}\n",
    "inv = {}\n",
    "for index, row in test.iterrows():\n",
    "    ts[(row[\"biker_id\"], row[\"tour_id\"])] = row[\"timestamp\"]\n",
    "    inv[(row[\"biker_id\"], row[\"tour_id\"])] = row[\"invited\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikers_out = []\n",
    "tours_out = []\n",
    "for biker1 in tqdm(test_bikers_set):\n",
    "    idx = np.where(biker1==test[\"biker_id\"]) \n",
    "    tour = list(test[\"tour_id\"].loc[idx]) # for each unique biker in test data get all the events  \n",
    "    score = {}\n",
    "    for tou in tour:\n",
    "        fe = make_feature(biker1, tou, ts[(biker1, tou)], inv[(biker1, tou)]).reshape(1,-1)\n",
    "        s1 = clf0.predict_proba(fe)[0][1]\n",
    "        s2 = clf1.predict(fe)[0]\n",
    "        #print(s1, s2)\n",
    "        score[tou] = (s1 + s2)/2\n",
    "    #print(score)\n",
    "    tour.sort(key = lambda x : score[x], reverse = True)\n",
    "    tour = \" \".join(tour) # list to space delimited string\n",
    "    bikers_out.append(biker1)\n",
    "    tours_out.append(tour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission =pd.DataFrame(columns=[\"biker_id\",\"tour_id\"])\n",
    "sample_submission[\"biker_id\"] = bikers_out\n",
    "sample_submission[\"tour_id\"] = tours_out\n",
    "sample_submission.to_csv(dire + \"/submission.csv\",index=False) # download this file from /kaggle/working directory\n",
    "print(sample_submission.shape)\n",
    "print(sample_submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
